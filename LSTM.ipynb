{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RRCh_kzCKBk",
        "outputId": "fd5f5bdd-0ab3-4360-f9ac-3eacb7929d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: hexbytes in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "!pip install datasets\n",
        "!pip install hexbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YnR1BlWMBl6X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchinfo import summary\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from argparse import ArgumentParser\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import normalize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from hexbytes import HexBytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS8QHYZlCIv7",
        "outputId": "b98c855e-d471-4fe2-bab8-5bbe59bd7fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2487: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for mwritescode/slither-audited-smart-contracts contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mwritescode/slither-audited-smart-contracts\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_ds = load_dataset(\"mwritescode/slither-audited-smart-contracts\", 'big-multilabel', split='train', ignore_verifications=True)\n",
        "val_ds = load_dataset(\"mwritescode/slither-audited-smart-contracts\", 'big-multilabel', split='validation', ignore_verifications=True)\n",
        "test_ds = load_dataset(\"mwritescode/slither-audited-smart-contracts\", 'big-multilabel', split='test', ignore_verifications=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XHtjlOY4CzQC"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.filter(lambda elem: elem['bytecode'] != '0x')\n",
        "val_ds = val_ds.filter(lambda elem: elem['bytecode'] != '0x')\n",
        "test_ds = test_ds.filter(lambda elem: elem['bytecode'] != '0x')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xQRsx746EH3D"
      },
      "outputs": [],
      "source": [
        "SAFE_IDX = 4 # the index of safe smart contract\n",
        "\n",
        "def __get_one_hot_encoded_label(label):\n",
        "    one_hot = np.zeros(5)\n",
        "    for elem in label:\n",
        "        if elem < SAFE_IDX:\n",
        "            one_hot[elem] = 1\n",
        "        elif elem > SAFE_IDX:\n",
        "            one_hot[elem-1] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def generate_signal_and_label(example):\n",
        "    code = HexBytes(example['bytecode']) # convert from string to bytes\n",
        "    image = np.frombuffer(code, dtype=np.uint8)\n",
        "    example['image'] = image\n",
        "    example['label'] = __get_one_hot_encoded_label(example['slither'])\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aEQzlgUCC4eG"
      },
      "outputs": [],
      "source": [
        "map_func = generate_signal_and_label\n",
        "\n",
        "train_ds = train_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])\n",
        "val_ds = val_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])\n",
        "test_ds = test_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mb3CQQAHEiog"
      },
      "outputs": [],
      "source": [
        "max_len = 512\n",
        "padding_val = 0\n",
        "\n",
        "def img_label_to_tensor(examples):\n",
        "  if 'image' in examples.keys():\n",
        "      examples['image'] = [np.pad(img, pad_width=(0, max_len - len(img)), constant_values=padding_val) if len(img) < max_len else img[:max_len] for img in examples['image']]\n",
        "      examples['image'] = [torch.tensor(img) for img in examples['image']]\n",
        "  if 'label' in examples.keys():\n",
        "      examples['label'] = torch.tensor(examples['label'])\n",
        "      return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vuNCSFm_FTHh"
      },
      "outputs": [],
      "source": [
        "train_ds.set_transform(img_label_to_tensor)\n",
        "val_ds.set_transform(img_label_to_tensor)\n",
        "test_ds.set_transform(img_label_to_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6YukXmQEFb3_"
      },
      "outputs": [],
      "source": [
        "model_name = 'lstm'\n",
        "num_cls = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "prp2TgO5FhdS"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "loader_train = DataLoader(train_ds,\n",
        "                    batch_size=batch_size,\n",
        "                    drop_last=True,\n",
        "                    shuffle=True)\n",
        "loader_val = DataLoader(val_ds,\n",
        "                    batch_size=batch_size,\n",
        "                    drop_last=True,\n",
        "                    shuffle=False)\n",
        "loader_test = DataLoader(test_ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "et8LFetZVofV"
      },
      "outputs": [],
      "source": [
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=1, classify=True, vocabulary_size=257):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        self.layers = 3\n",
        "        self.hidden_size = 128\n",
        "        self.classify = classify\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=150, padding_idx=256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.lstm = nn.LSTM(bidirectional=True, input_size=150, hidden_size=self.hidden_size, batch_first=True, num_layers=self.layers)\n",
        "        self.dense1 = nn.Linear(in_features=256, out_features=512)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        if classify:\n",
        "            self.dense2 = nn.Linear(in_features=512, out_features=num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        lenghts = inputs.shape[1] - (inputs == 256).sum(dim=1).to('cpu')\n",
        "        out = self.dropout1(self.embedding(inputs))\n",
        "\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lenghts, batch_first=True, enforce_sorted=False)\n",
        "        _, (h_n, _) = self.lstm(out)\n",
        "        h_n = h_n.view(self.layers, 2, inputs.shape[0], self.hidden_size)\n",
        "        last_hidden = h_n[-1]\n",
        "        last_hidden_fwd = last_hidden[0]\n",
        "        last_hidden_bwd = last_hidden[1]\n",
        "        out = torch.cat((last_hidden_fwd, last_hidden_bwd), 1)\n",
        "\n",
        "        out = self.dropout2(self.relu(self.dense1(out)))\n",
        "        if self.classify:\n",
        "            out = self.dense2(out)\n",
        "        return out\n",
        "\n",
        "    def get_layer_groups(self):\n",
        "        linear_layers = [elem[1] for elem in self.dense2.named_parameters()]\n",
        "        other_layers = [elem[1] for elem in filter(lambda param_tuple: 'dense2' not in param_tuple[0], self.named_parameters())]\n",
        "        param_groups = {\n",
        "            'classifier': linear_layers,\n",
        "            'feature_extractor': other_layers\n",
        "        }\n",
        "        return param_groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NYhWNUDKEr2i"
      },
      "outputs": [],
      "source": [
        "model = LSTMNetwork(num_classes=num_cls)\n",
        "model = model.to('cuda')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0xfxNXE_VpEz"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uJr4oVlWCx2",
        "outputId": "28249591-e07e-469d-d9f9-f2f99952b3c8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:50<00:00,  3.02it/s, loss=1.87, acc=0.143]\n",
            "Valing...: 100%|██████████| 169/169 [00:56<00:00,  3.01it/s, loss=1.81, acc=0.171]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss: 1.8750 | val_loss: 1.8144 | train_acc: 0.1426 | val_acc: 0.1710\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:48<00:00,  3.03it/s, loss=1.77, acc=0.19]\n",
            "Valing...: 100%|██████████| 169/169 [00:52<00:00,  3.25it/s, loss=1.73, acc=0.21]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss: 1.7696 | val_loss: 1.7346 | train_acc: 0.1895 | val_acc: 0.2099\n",
            "\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:35<00:00,  3.13it/s, loss=1.71, acc=0.219]\n",
            "Valing...: 100%|██████████| 169/169 [00:50<00:00,  3.33it/s, loss=1.7, acc=0.237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.7133 | val_loss: 1.7004 | train_acc: 0.2191 | val_acc: 0.2367\n",
            "\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:35<00:00,  3.13it/s, loss=1.67, acc=0.24]\n",
            "Valing...: 100%|██████████| 169/169 [00:50<00:00,  3.34it/s, loss=1.68, acc=0.259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.6692 | val_loss: 1.6837 | train_acc: 0.2397 | val_acc: 0.2594\n",
            "\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:37<00:00,  3.12it/s, loss=1.63, acc=0.26]\n",
            "Valing...: 100%|██████████| 169/169 [00:50<00:00,  3.33it/s, loss=1.66, acc=0.279]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.6315 | val_loss: 1.6600 | train_acc: 0.2596 | val_acc: 0.2786\n",
            "\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:36<00:00,  3.13it/s, loss=1.59, acc=0.277]\n",
            "Valing...: 100%|██████████| 169/169 [00:52<00:00,  3.25it/s, loss=1.64, acc=0.275]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.5944 | val_loss: 1.6443 | train_acc: 0.2767 | val_acc: 0.2751\n",
            "\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:36<00:00,  3.13it/s, loss=1.56, acc=0.29]\n",
            "Valing...: 100%|██████████| 169/169 [00:50<00:00,  3.34it/s, loss=1.64, acc=0.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.5591 | val_loss: 1.6412 | train_acc: 0.2901 | val_acc: 0.2804\n",
            "\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:34<00:00,  3.14it/s, loss=1.52, acc=0.302]\n",
            "Valing...: 100%|██████████| 169/169 [00:50<00:00,  3.33it/s, loss=1.64, acc=0.292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.5233 | val_loss: 1.6354 | train_acc: 0.3021 | val_acc: 0.2915\n",
            "\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:37<00:00,  3.12it/s, loss=1.49, acc=0.315]\n",
            "Valing...: 100%|██████████| 169/169 [00:51<00:00,  3.31it/s, loss=1.64, acc=0.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.4924 | val_loss: 1.6425 | train_acc: 0.3153 | val_acc: 0.2898\n",
            "\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...: 100%|██████████| 1240/1240 [06:37<00:00,  3.12it/s, loss=1.46, acc=0.316]\n",
            "Valing...: 100%|██████████| 169/169 [00:52<00:00,  3.23it/s, loss=1.64, acc=0.295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 1.4619 | val_loss: 1.6390 | train_acc: 0.3157 | val_acc: 0.2953\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valing...: 100%|██████████| 249/249 [01:16<00:00,  3.27it/s, loss=0.98, acc=0.174]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "metrics = {'acc': accuracy_score}\n",
        "\n",
        "def initialize_logs_dict(loader_train, loader_val):\n",
        "    logs = {\n",
        "        'epoch_num': 0,\n",
        "        'train_batches_per_epoch': len(loader_train),\n",
        "        'val_batches_per_epoch': len(loader_val) if loader_val is not None else None,\n",
        "        'train': {'loss': 0.0, 'predictions': [], 'labels': [], 'batch_idx': 0},\n",
        "        'val': {'loss': 0.0, 'predictions': [], 'labels': [], 'batch_idx': 0},\n",
        "        'metrics': {'train_' + metric: 0.0 for metric in metrics.keys()} | {'val_' + metric: 0.0 for metric in metrics.keys()}\n",
        "    }\n",
        "    return logs\n",
        "\n",
        "def run_epoch(model, criterion, optimizer, data_loader, device, mode, logs):\n",
        "    model.train() if mode == 'train' else model.eval()\n",
        "    total_loss = 0.0\n",
        "    running_metrics = {metric: 0.0 for metric in metrics.keys()}\n",
        "    pbar = tqdm(data_loader, desc=f'{mode.capitalize()}ing...')\n",
        "\n",
        "    for data in pbar:\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(mode == 'train'):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = (outputs >= 0.0).float()\n",
        "            logs[mode]['predictions'] += preds.tolist()\n",
        "            logs[mode]['labels'] += labels.tolist()\n",
        "            logs[mode]['loss'] = total_loss / (logs[mode]['batch_idx'] + 1)\n",
        "\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                running_metrics[metric_name] += metric_func(labels.tolist(), preds.tolist())\n",
        "                logs['metrics'][mode + '_' + metric_name] = running_metrics[metric_name] / (logs[mode]['batch_idx'] + 1)\n",
        "\n",
        "            logs[mode]['batch_idx'] += 1\n",
        "            pbar.set_postfix({'loss': logs[mode]['loss'], **{metric_name: logs['metrics'][mode + '_' + metric_name] for metric_name in metrics.keys()}})\n",
        "\n",
        "            if mode == 'train':\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "def main_training_loop(model, criterion, optimizer, loader_train, loader_val, device, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch}:')\n",
        "        logs = initialize_logs_dict(loader_train, loader_val)\n",
        "        logs['epoch_num'] = epoch\n",
        "\n",
        "        run_epoch(model, criterion, optimizer, loader_train, device, 'train', logs)\n",
        "        run_epoch(model, criterion, optimizer, loader_val, device, 'val', logs)\n",
        "\n",
        "        print('train_loss: {:.4f} | val_loss: {:.4f} |'.format(logs['train']['loss'], logs['val']['loss']), end=' ')\n",
        "        print(\" | \".join(['{}: {:.4f}'.format(metric_name, metric_val) for metric_name, metric_val in logs['metrics'].items()]), end='\\n\\n')\n",
        "\n",
        "    run_epoch(model, criterion, optimizer, loader_test, device, 'val', logs)\n",
        "\n",
        "\n",
        "main_training_loop(model, criterion, optimizer, loader_train, loader_val, device, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BebWlEtK4R4M"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}